---
title: 目标检测疏理
mathjax: true
date: 2020-08-05 14:15:50
tags:
categories:
- 目标检测
---





博客：[博客园](https://www.cnblogs.com/shine-lee/) | [CSDN](https://blog.csdn.net/blogshinelee) | [blog](https://blog.shinelee.me/)



Two-stage：Stage1-Localization（Region Proposal），Stage2-Classification

One-stage：

两阶段延续的是经典的目标检测流程，

Region Proposal，Feature Extraction，Classification，BBox Regression

结论：

- end to end 训练更好
- multi-task 更好
- finetune更好

问题：

- 



- RCNN：Selected Search+AlexNet+SVM

  - Selected Search：基于颜色、纹理、形状、梯度等，计算相似度的区域生长与合并，生成BBox，保证GT BBox的召回率
  - Train：fc7+Finetune+BBox Regression
    - resize input image to 227x227
    - 训练特征提取器的类别不均衡问题：crop后控制mini batch中正负比例为1:3
    - 训练分类器
    - 一个类别一个BBox Regressor：POOL5特征作为输入
    - NMS
  - 问题：
    - 非端到端
    - 每个子图片都要单独存储，且都需要一次前向+SVM+BBox

- SPPNet：Spatial Pyramid Pooling

  - 子图像有重叠部分，重复计算，image crop from input image→ feature crop from convolution layer，在feature map上扣子图，速度加快，不需要存储子图
  - SPP layer的反向传播没有研究透，

- Fast RCNN：将

  - ROI Pooling Layer，代替SPP Layer，可反向传播
  - ROI：all resize to 5x5，
  - pos：IOU>0.5，neg：0.5>IOU>0.1
  - mini batch：2 image 128 Proposal 1:3
  - multi task loss

- Faster RCNN：真正意义上的端到端，首次做到时时

  - RPN（定义了Anchor）：替换Selected Search，将Region Proposal融入端到端训练pipeline。
    - 在feature map上以3x3做sliding window，结果为256x13x13，两个1x1的网络cls和reg，cls只做前背景判断，reg回归出中心点偏移和宽高
    - 1个位置预测9个候选框，候选框的计算依赖于初值（Anchor）和偏移量（RPN的输出），
    - 9个Anchor可以分成3组，每组内的Anchor面积相同，长宽比不同
  - 速度提升：测试阶段，RPN产生了

- Mask RCNN

  - ROI Align
  - Mask Loss

- YOLO-V1（2015）：速度快；看全图的信息；

  - 假设一张图中最多有K个物体，输出

  - ……1024x7x7 feature map 经全连接层 4096 再经全连接层 30x7x7，先**打破位置对应关系，融合信息，再恢复回来**（后面版本被舍弃），30 = （2*5 + 20），5：dx、dy、dw、dh、conf（前景 x IOU），有没有物体，20个类别条件概率，

  - Object in a cell：GT BBox的中心位于哪个cell，不会跑出当前cell

  - BBox response for object：当训练时产生的2个BBox

  - Loss：

    ![image-20200805201118207](C:\Users\Thinkpad\AppData\Roaming\Typora\typora-user-images\image-20200805201118207.png)

  - 没有进行正负样本划分，通过权重来控制，定位误差权重5，判定有没有物体的权重为0.5,
  - 问题：
    - 小物体检测不好，全图划分成7x7的cell grid，邻近的小物体可能落在同一个cell里
    - 对大小物体同等看待，小物体对位置偏移更敏感（应该被更看重）

- YOLO-V2（2016）

  - 抛弃了YOLO-V1的全连接层

  - Backbone

    - 移除最初的7x7 stride2的，采用小核卷积
    - BN
    - High Resolution Classifier

  - Detector

    - 仿SSD：用不同层的feature map信息，前面层的定位信息更好
    - 使用Anchor：统计所有GT的BBox并聚类，#Anchor

  - 训练

    - multi-scale training：对不同尺寸的物体有更好的识别能力

  - 优化方法

    ![image-20200805204251507](C:\Users\Thinkpad\AppData\Roaming\Typora\typora-user-images\image-20200805204251507.png)

- YOLO-V3

  - FPN
    - 越深层语义信息越多，越浅层位置信息越多
  - Backbone：Resnet
  - Detector
    - FPN：深层预测大的检测框，浅层预测小的检测框
    - Anchor clustering 9
  - Train
    - Binary Cross Entropy Loss：多标签分类任务
  - 问题
    - BBox小的损失更大，降低大的BBox对网络的影响





# 参考

- [SIGAI-目标检测]([http://www.tensorinfinity.com/index.php?r=front/knowledgelist&cata=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89](http://www.tensorinfinity.com/index.php?r=front/knowledgelist&cata=计算机视觉))
- 