---
title: 轻量化网络回顾
mathjax: true
date: 2020-04-09 16:53:03
tags:
categories:
- backbone网络
---



博客：[博客园](https://www.cnblogs.com/shine-lee/) | [CSDN](https://blog.csdn.net/blogshinelee) | [blog](https://blog.shinelee.me/)



# 写在前面

深度神经网络的演化过程是在计算资源与准确率之间寻求平衡的过程。可以把计算资源与准确率（或其他指标）看做两个坐标轴，一个极端是追求极致的准确率而相对忽视计算资源的消耗，另一个极端是在有限的计算资源下寻求最好的准确率，前者动辄数百上千层，需要数BFLOPs，后者的硬件环境主要为智能硬件、机器人等，可能仅有数百MFLOPs。

为了适应有限的计算资源，有很多方法，比如调整网络结构、知识蒸馏、模型量化、压缩、加速等，实际应用中可以是多种方法的结合。

**本文讨论的轻量化网络，其重心在通过优化网络结构，以在有限的计算资源约束下获得更好的准确率。**轻量化网络关注的指标主要有3个，

- **参数量#Params**
- **计算量FLOPS** 
- **准确率Accuracy**

早期CNN的参数主要集中在全连接层，而计算主要集中在卷积层，若采用Global Pooling取代掉全连接层，全连接层参数量大大减少，所以**主要关注卷积层的参数量和计算量，两者通常是正相关的**。

# 形式化

方便起见，对常规卷积操作，做如下定义，

- $I$：输入尺寸，长$H$ 宽$W$ ，令长宽相同，即$I = H = W$
- $M$：输入channel数，可以看成是tensor的高
- $K$：卷积核尺寸$K \times K$，channel数与输入channel数相同，为$M$
- $N$：卷积核个数
- $F$：卷积得到的feature map尺寸$F \times F$，channel数与卷积核个数相同，为$N$

所以，输入为$M \times I \times I$的tensor，卷积核为$N \times M \times K \times K$的tensor，feature map为$N \times F \times F$的tensor，所以常规卷积的计算量为
$$
FLOPS = K \times K \times M \times N \times F \times F
$$
特别地，如果仅考虑SAME padding且$stride = 1$的情况，则$F = I$，则计算量等价为
$$
FLOPS = K \times K \times M \times N \times I \times I
$$
可以看成是$(K \times K \times M) \times (N \times I \times I)$，前一个括号为卷积中一次内积运算的计算量，后一个括号为需要多少次内积运算。

参数量为
$$
\#Params = N \times M \times K \times K
$$


# 总览网络演化

总览SqueezeNet、MobileNet V1 V2、ShuffleNet等各种轻量化网络，可以看成对卷积核$M \times K \times K$ 做各种分解（同时加入激活函数），并调整卷积核数量$N$，以获得性能和计算量的平衡，从FLOPS上看，相当于对$K \times K \times M \times N \times I \times I$做了各种变换。

下面就通过这个视角进行一下疏理，只列出其中发生改变的因子项，

- **Group Convolution（AlexNet）**，相当于
  $$
  M \rightarrow \frac{M}{G}
  $$

- **大卷积核替换为多个堆叠的小核（VGG）**，比如$5\times 5$替换为2个$3\times 3$，相当于
  $$
  (K \times K) \rightarrow (k \times k + k \times k)
  $$

- **Factorized Convolution（Inception V2）**，二维卷积变为行列分别卷积，相当于将长宽分解出去
  $$
  (K \times K) \rightarrow (K \times 1 + 1 \times K)
  $$

- **Fire module（SqueezeNet）**，pointwise+ReLU+(pointwise + 3x3 conv)+ReLU，pointwise降维，同时将一定比例的$3\times 3$卷积替换为为$1 \times 1$，
  $$
  (K \times K \times M \times N) \rightarrow (M \times \frac{N}{t} + \frac{N}{t} \times (1-p)N + \frac{N}{t} \times pN)
  $$

- **Bottleneck（ResNet）**，**pointwise+ReLU+standard+ReLU+pointwise**，相当于对channel维做SVD，
  $$
  (K \times K \times M \times N) \rightarrow (M \times \frac{N}{t} + K \times K \times \frac{N}{t} \times \frac{N}{t} + \frac{N}{t} \times N) \\
  t = 4
  $$

- **ResNeXt Block（ResNeXt）**，相当于引入了group  $3\times 3$ convolution的bottleneck，
  $$
  (K \times K \times M \times N) \rightarrow (M \times \frac{N}{t} + K \times K \times \frac{N}{tG} \times \frac{N}{t} + \frac{N}{t} \times N) \\
  t = 2, \ G = 32
  $$

- **Depthwise Separable Convolution（MobileNet V1）**，**depthwise + ReLU + pointwise + ReLU**，相当于将channel维单独分解出去，
  $$
  (K \times K \times N) \rightarrow (K \times K + N)
  $$

- **Separable Convolution（Xception）**，**pointwise + depthwise + ReLU**，也相当于将channel维分解出去，但前后顺序不同，同时移除了两者间的ReLU，
  $$
  (K \times K \times M) \rightarrow (M + K \times K)
  $$


- **pointwise group convolution and channel shuffle（ShuffleNet）**，pointwise group +ReLU+Channel Shuffle+depthwise separable，相当于bottleneck中2个pointwise引入相同的group，同时$3\times 3$ conv变成depthwise，也就是说3层卷积都group了，这会阻碍不同channel间的信息交流，所以在第一个pointwise后加入了channel shuffle，

  $$
  (K \times K \times M \times N) \rightarrow (\frac{M}{G} \times \frac{N}{t} + channel \ shuffle +K \times K \times \frac{N}{t} + \frac{N}{tG} \times N)
  $$
  
- **Inverted Linear Bottleneck（MobileNet V2）**，bottleneck是先通过pointwise降维、再卷积、再升维，Inverted bottleneck是先升维、再卷积、再降维，pointwise+ReLU6+depthwise+ReLU6+pointwise，
  $$
  (K \times K \times M \times N) \rightarrow (M \times tM + K \times K \times tM  + tM \times N)
  $$

上述所列，全为

# SqueezeNet

squeeze≈bottleneck

Fire Module: 1x1 conv → 1x1 conv + 3x3 conv，Squeeze → expand + expand

Deep Compression

DESIGN SPACE EXPLORATION：网络模块化，模块参数化

# MobileNet-V1

depthwise separable convolution = depthwise conv + BN + ReLU + pointwise conv + BN + ReLU

整个网络采用 depthwise separable convolution

Width Multiplier  + Resolution Multiplier  trading off a reasonable amount of accuracy to reduce size and latency.  



# Xception

depthwise separable convolution = depthwise conv + ReLU + pointwise conv + ReLU

这篇提出的 SeparableConvolution = pointwise conv + depthwise conv + ReLU，其灵感来自 “extreme” version of an Inception module，Xception stands for “Extreme Inception”



# ShuffleNet

# SENet

# MobileNet-V2



# 参考

- [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)